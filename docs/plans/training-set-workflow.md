# Training Set Integration & Workflow

**Date:** 2026-01-25
**Context:** Clarification on how training/validation/held-out sets are used in optimization
**Related:** [Implementation Proposal](./2026-01-25-iterative-refinement-implementation.md)

## Key Clarifications

### Data Split Strategy

**Training Set:**
- Input/output pairs showing desired task behavior
- Used **during optimization** to evaluate candidate prompts
- Provides feedback signal for iterative refinement
- Larger is better (more signal for pattern detection)

**Validation Set:**
- Input/output pairs used during optimization with descriptive feedback
- **Seen by the optimizer only as patterns, not specific examples**
- Used to improve generalization without overfitting

**Held-out Test Set:**
- Held-out input/output pairs
- **Never used during optimization**
- Evaluated once at the end for true out-of-sample performance

**Final Prompt:**
- Zero-shot instructions (no examples embedded)
- Should work on new cases without demonstrations
- Evaluated against training and validation sets during optimization, and held-out at the end

---

## Updated Optimization Workflow

### Phase 1: Initial Prompt Generation

```
Input:
- Task description from user
- Training set examples (to understand task requirements)

Process:
- Optimizer (Opus) analyzes training examples
- Identifies patterns in inputs/outputs
- Generates initial zero-shot instruction prompt

Output:
- Draft zero-shot prompt (Iteration 0)
```

**Example:**
```
Task: "Classify sentiment of product reviews"

Training Examples Shown to Opus:
1. "This product is amazing!" → "positive"
2. "Broke after one day" → "negative"
3. "It's okay, nothing special" → "neutral"
...

Initial Prompt Generated by Opus:
"Classify the sentiment of the following product review as positive, negative, or neutral.
Consider the overall emotional tone and satisfaction level expressed by the reviewer.
Output only the sentiment label in lowercase."
```

---

### Phase 2: Optimization Loop

```
For each iteration:

1. TEST RUNNER
   - Execute current prompt against ALL training set cases
   - Capture actual outputs

2. FEEDBACK ANALYZER
   - Compute metrics:
     * Pass rate: 15/20 (75%)
     * Per-case results with diffs

   - Categorize failures:
     * Error Type A: Format issues (2 cases)
     * Error Type B: Boundary confusion (3 cases)

   - Generate detailed feedback:
     Training Case 7:
       Input: "Product is adequate but overpriced"
       Expected: "neutral"
       Got: "negative"
       Diff: neutral vs negative
       Analysis: Instruction unclear about mixed sentiments

3. OPTIMIZER (Opus)
   - Receives comprehensive feedback
   - Analyzes failure patterns across training set
   - Identifies systematic issues:
     * "3/5 failures involve mixed sentiments → add guidance"
     * "2/5 failures are format issues → strengthen format spec"

   - Refines prompt:
     "Classify the sentiment of the following product review as positive,
     negative, or neutral.

     Guidelines:
     - Positive: Overall satisfaction, would recommend
     - Negative: Dissatisfaction, problems, would not recommend
     - Neutral: Mixed feelings, adequate but unremarkable, factual statements

     For mixed sentiments (e.g., 'good but expensive'), determine which
     aspect dominates the reviewer's overall opinion.

     Output ONLY the sentiment label in lowercase: positive, negative, or neutral"

4. CONVERGENCE CHECK
   - All training cases pass? → Proceed to validation
   - Improvement plateau? → Manual review
   - Max iterations reached? → Return best so far
   - Otherwise → Loop back to step 1
```

---

### Phase 3: Held-out Test Evaluation

```
Once training + validation pass:

1. Run optimized prompt against held-out TEST SET
2. Compute held-out metrics
3. Report both:
   - Training performance: 20/20 (100%)
   - Held-out performance: 18/20 (90%)

If held-out performance is significantly lower:
→ Prompt overfit to training set
→ May need regularization or more diverse training data
```

---

## How Training Set Strengthens Our Approach

### Benefits for Iterative Refinement (Approach 1)

1. **Rich Feedback Signal**
   - Multiple failure cases reveal patterns
   - Opus can identify systematic issues vs random errors
   - Example: "5/20 cases involve mixed sentiments" → clear guidance needed

2. **Pattern Detection**
   - Training set size enables error categorization
   - Feedback analyzer can cluster similar failures
   - Helps Opus prioritize what to fix

3. **Generalization Testing**
   - Zero-shot prompts must generalize beyond training examples
   - Test set validates this isn't just memorization
   - Larger training set → better generalization signal

4. **Iterative Improvement**
   - Each iteration shows progress on training metrics
   - Clear feedback loop: 60% → 75% → 90% → 100%
   - Detailed diffs show exactly what improved

### Why DSPy Doesn't Become More Relevant

**Common Misconception:** "Training set means DSPy is perfect!"

**Reality:**
- DSPy's `BootstrapFewShot`: Selects examples to **embed in prompt** ❌ (We want zero-shot)
- DSPy's `MIPROv2`/`COPRO`: Optimize instructions ✅ (But still have other issues)

**Remaining DSPy Limitations:**
- Feedback integration: DSPy optimizers use scalar metrics, we have rich structured feedback
- Framework lock-in: Optimized prompts tied to DSPy runtime
- Our use case: Single-stage zero-shot optimization, not DSPy's sweet spot (multi-stage pipelines)

**Training set helps both approaches equally:**
- Approach 1 (Custom): Training set → evaluation → rich feedback → Opus refinement
- Approach 2 (DSPy): Training set → DSPy metric function → optimizer search

But Approach 1's rich feedback integration remains the key advantage.

---

## Updated Architecture Diagram

```
┌────────────────────────────────────────────────────────┐
│                    USER INPUT                          │
│  - Task description                                    │
│  - Training set (input/output pairs)                   │
│  - Test set (held-out)                                 │
└─────────────────────┬──────────────────────────────────┘
                      │
                      ▼
         ┌────────────────────────────┐
         │  INITIAL PROMPT GENERATION │
         │  (Opus analyzes training   │
         │   examples, generates      │
         │   zero-shot instructions)  │
         └──────────┬─────────────────┘
                    │
                    ▼
         ┌──────────────────────────────────┐
         │   OPTIMIZATION LOOP              │
         │   (on training set)              │
         │                                  │
         │  ┌────────────────────────┐      │
         │  │ 1. Test on training    │      │
         │  │    set (20 cases)      │      │
         │  └───────────┬────────────┘      │
         │              │                   │
         │              ▼                   │
         │  ┌────────────────────────┐      │
         │  │ 2. Analyze failures    │      │
         │  │    - Diffs             │      │
         │  │    - Error categories  │      │
         │  │    - Pattern detection │      │
         │  └───────────┬────────────┘      │
         │              │                   │
         │              ▼                   │
         │  ┌────────────────────────┐      │
         │  │ 3. Opus refines prompt │      │
         │  │    based on patterns   │      │
         │  └───────────┬────────────┘      │
         │              │                   │
         │              ▼                   │
         │       ┌─────────────┐            │
         │       │ 100% pass?  │            │
         │       └──┬────────┬─┘            │
         │          │ No     │ Yes          │
         └──────────┼────────┼──────────────┘
                    │        │
                    └─Loop   │
                             ▼
                  ┌──────────────────────┐
                  │  HELD-OUT EVAL       │
                  │  (held-out set)      │
                  │                      │
                  │  Train: 20/20 (100%) │
                  │  Held-out: 18/20 (90%)│
                  └──────────────────────┘
                             │
                             ▼
                  ┌──────────────────────┐
                  │  RETURN OPTIMIZED    │
                  │  ZERO-SHOT PROMPT    │
                  └──────────────────────┘
```

---

## Configuration Format Update

### Updated YAML Format

```yaml
# config.yaml
task_description: |
  Classify sentiment of product reviews as positive, negative, or neutral.

target_model:
  provider: anthropic
  model: claude-sonnet-4.5
  temperature: 0.0  # Deterministic for classification

optimizer:
  model: claude-opus-4.5
  max_iterations: 20
  plateau_threshold: 5

data:
  training_set: ./data/sentiment_train.jsonl
  validation_set: ./data/sentiment_validation.jsonl
  heldout_set: ./data/sentiment_test.jsonl

# Training set format (sentiment_train.jsonl):
# {"input": "This product is amazing!", "expected_output": "LABEL=POSITIVE"}
# {"input": "Broke after one day", "expected_output": "LABEL=NEGATIVE"}
# ...

stopping_criteria:
  training_pass_rate: 1.0  # Must pass 100% of training set
  max_iterations: 20
  plateau_iterations: 5
```

---

## Key Metrics to Track

### During Optimization (Training Set)
- **Pass Rate:** % of training cases passing
- **Error Distribution:** Count by error category
- **Improvement Rate:** ΔPass rate per iteration
- **Convergence Trend:** Are we improving or plateauing?

### Final Evaluation (Held-out Test Set)
- **Test Pass Rate:** % of held-out cases passing
- **Generalization Gap:** Train pass rate - Test pass rate
  - Small gap (< 10%): Good generalization ✅
  - Large gap (> 20%): Overfitting to training set ⚠️

### Success Criteria
- **Primary:** Training set 100% + Test set ≥ 90%
- **Secondary:** Achieved in ≤ 15 iterations
- **Quality:** Test set performance indicates real-world readiness

---

## Example: Full Workflow

### Initial State

**Training Set (20 cases):**
- 20 product review → sentiment pairs
- Covering edge cases: mixed sentiments, sarcasm, neutral statements

**Validation Set (5 cases):**
- Used during optimization with descriptive feedback

**Held-out Test Set (5 cases):**
- Evaluated once at the end (never used during optimization)

### Iteration 0: Initial Prompt

```
Opus receives:
- Task: "Classify sentiment"
- Training examples (sees all 20)

Generates:
"Classify the sentiment: positive, negative, or neutral."

Results:
- Training: 12/20 (60%)
- Issues: Too simple, no guidance for edge cases
```

### Iteration 1: First Refinement

```
Feedback to Opus:
- 8 failures:
  * 3 mixed sentiments → classified incorrectly
  * 2 sarcasm → missed tone
  * 3 format errors → wrong format

Opus refines:
"Classify sentiment of the review as positive, negative, or neutral.
Consider overall satisfaction, not individual aspects.
Output only: positive, negative, or neutral (lowercase)."

Results:
- Training: 17/20 (85%)
- Improvement: +25%
- Remaining: 3 sarcasm cases
```

### Iteration 2: Second Refinement

```
Feedback to Opus:
- 3 failures (sarcasm):
  * "Oh great, another broken product" → classified as positive (missed sarcasm)

Opus refines:
"Classify sentiment...
Pay attention to context: sarcasm or irony indicates negative sentiment.
Phrases like 'Oh great...' or 'Just what I needed...' are typically sarcastic."

Results:
- Training: 20/20 (100%) ✅
- Proceed to validation
```

### Final Validation

```
Test set evaluation:
- Test: 4/5 (80%)
- 1 failure: Novel edge case not in training

Decision:
- 80% test performance acceptable? Or add to training and re-optimize?
```

---

## Why This Workflow Fits Approach 1 Perfectly

1. **Rich Training Signal:** More cases = better pattern detection for Opus
2. **Detailed Feedback:** Per-case diffs + error categories guide refinement
3. **Iterative Refinement:** Each iteration targets specific failure patterns
4. **Generalization Focus:** Test set validates prompt works beyond training
5. **Transparent Process:** Clear metrics at each step

This workflow would be **harder with DSPy** because:
- DSPy optimizers want aggregate metrics, not per-case detailed feedback
- Pattern detection and categorized errors don't fit DSPy's optimization primitives
- We'd lose the transparency of "Opus reasoning about specific failure cases"

---

## Recommendation: Still Approach 1 (or Hybrid 1)

Training set **strengthens** the case for Approach 1:
- ✅ More data → better refinement signal
- ✅ Pattern detection across cases → intelligent optimization
- ✅ Clear train/test split → proper validation
- ✅ Zero-shot goal → no need for example selection (DSPy's strength)

**Final Score:**
- **Approach 1 (Custom):** 9/10 (excellent fit)
- **Hybrid 1 (Custom + LangSmith):** 9/10 (adds observability)
- **Approach 2 (DSPy):** 4/10 (still architectural mismatch)

The training set doesn't change the fundamental architectural fit.
